{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angel-Castro-RC/Final_NLP/blob/main/F6_2_RecurrentNeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Recurrent Neural Networks\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_2_RecurrentNeuralNetworks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oSfZPDe9el0"
      },
      "source": [
        "## Announcement Update\n",
        "\n",
        "AI - English Faculty Candidate: Gabriel Ford\n",
        "\n",
        "Scholarly Presentation: Friday at 9:00am in Howard 309"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es4Xnlft9el1"
      },
      "source": [
        "## Reference\n",
        "\n",
        "SLP: RNNs and LSTMs, Chapter 9 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
        "\n",
        "Keras documentation for SimpleRNN Layer: https://keras.io/api/layers/recurrent_layers/simple_rnn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvQ6j8cf9el1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc31273-944d-498b-d2cc-b794aa9669f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/493.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/493.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: safetensors, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets keras tensorflow transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmMf6HUI9el2"
      },
      "source": [
        "## Recurrent Neural Networks (RNN)\n",
        "\n",
        "A **recurrent neural network** is a neural network with a loop inside of it - some of the outputs in one layer become inputs of the same or an earlier layer\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_highlevel.png?raw=1\">\n",
        "</div>\n",
        "\n",
        "* $x_{t}$: neural network input at time $t$\n",
        "* $h_{t}$: hidden layer state at time $t$\n",
        "* $y_{t}$: output layer state at time $t$\n",
        "\n",
        "*Allows information from past inputs to affect current predictions*\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.1, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28npsNwQ9el3"
      },
      "source": [
        "## RNN visualized as a feedforward network\n",
        "\n",
        "In this image, the inputs are shown on bottom and the outputs on top\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_as_feedforward.png?raw=1\" width=400>\n",
        "</div>\n",
        "\n",
        "* $h_{t-1}$: hidden layer state at time $t-1$ is an input to $h_{t}$\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.2, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4KGLh1n9el3"
      },
      "source": [
        "## RNN \"unrolled\" in time\n",
        "\n",
        "Later outputs continue to be influenced by the entire sequence\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_unroll.png?raw=1\" width=500>\n",
        "</div>\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.4, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx7lGmsI9el4"
      },
      "source": [
        "## Coding up a simple RNN in Keras\n",
        "\n",
        "Defining a Recurrent layer is similar to defining a Dense layer\n",
        "\n",
        "`return_sequences=False` for now, we don't want to return the entire sequence, just the last output\n",
        "\n",
        "`stateful=False` allows the state from one **batch** to carry over to the next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_lF_FSc9el4"
      },
      "outputs": [],
      "source": [
        "# A feedforward network with one hidden layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# A recurrent network with one layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(SimpleRNN(100,return_sequences=False,stateful=False))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVlA4hNk9el5"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Copy in your code from the non-recurrent neural language model from last time, and replace the Flatten+Dense layer with a SimpleRNN layer like above.\n",
        "* Use the same dataset, `ag_news`, prepared in the same way\n",
        "* Run it with small enough subset to train within a few minutes\n",
        "\n",
        "How do the performances compare?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "data_subset, _ = train_test_split(data[\"train\"][\"text\"], train_size=500)\n",
        "train_data, test_data = train_test_split(data_subset, train_size=0.8)\n",
        "\n",
        "# Prepare the tokenizer and fit on the training text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_subset)\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "train_texts = tokenizer.texts_to_sequences(train_data)\n",
        "\n",
        "sequence_length = 5  # Length of the input sequence before predicting the next word\n",
        "\n",
        "# Create the sequences\n",
        "predictor_sequences = []\n",
        "targets = []\n",
        "for text in train_texts:\n",
        "    for i in range(sequence_length, len(text)):\n",
        "        # Take the sequence of tokens as input and the next token as target\n",
        "        curr_target = text[i]\n",
        "        curr_predictor_sequence = text[i - sequence_length:i]\n",
        "        predictor_sequences.append(curr_predictor_sequence)\n",
        "        targets.append(curr_target)\n",
        "\n",
        "predictor_sequences = np.array(predictor_sequences)\n",
        "targets = np.array(targets)\n",
        "\n",
        "# Convert targets to one-hot vectors\n",
        "targets_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
        "\n",
        "# Create the neural language model with SimpleRNN layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(SimpleRNN(100, activation='relu'))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(predictor_sequences, targets_one_hot, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_texts = tokenizer.texts_to_sequences(test_data)\n",
        "test_predictor_sequences = []\n",
        "test_targets = []\n",
        "for text in test_texts:\n",
        "    for i in range(sequence_length, len(text)):\n",
        "        curr_target = text[i]\n",
        "        curr_predictor_sequence = text[i - sequence_length:i]\n",
        "        test_predictor_sequences.append(curr_predictor_sequence)\n",
        "        test_targets.append(curr_target)\n",
        "\n",
        "test_predictor_sequences = np.array(test_predictor_sequences)\n",
        "test_targets = np.array(test_targets)\n",
        "\n",
        "test_targets_one_hot = to_categorical(test_targets, num_classes=vocabulary_size)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_predictor_sequences, test_targets_one_hot)\n",
        "print(f\"Test accuracy: {accuracy }\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvcZ4pKfC1nR",
        "outputId": "75d1e4d1-388d-494d-8d8e-b51c44e6b75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5287\n",
            "Epoch 1/10\n",
            "341/341 [==============================] - 11s 27ms/step - loss: 7.6675 - accuracy: 0.0483 - val_loss: 7.5479 - val_accuracy: 0.0433\n",
            "Epoch 2/10\n",
            "341/341 [==============================] - 6s 16ms/step - loss: 7.0499 - accuracy: 0.0501 - val_loss: 7.7718 - val_accuracy: 0.0433\n",
            "Epoch 3/10\n",
            "341/341 [==============================] - 8s 23ms/step - loss: 6.7704 - accuracy: 0.0548 - val_loss: 7.8650 - val_accuracy: 0.0506\n",
            "Epoch 4/10\n",
            "341/341 [==============================] - 5s 15ms/step - loss: 6.4219 - accuracy: 0.0619 - val_loss: 7.9967 - val_accuracy: 0.0594\n",
            "Epoch 5/10\n",
            "341/341 [==============================] - 4s 11ms/step - loss: 6.0509 - accuracy: 0.0771 - val_loss: 8.7966 - val_accuracy: 0.0631\n",
            "Epoch 6/10\n",
            "341/341 [==============================] - 6s 16ms/step - loss: 5.6293 - accuracy: 0.0960 - val_loss: 9.0141 - val_accuracy: 0.0708\n",
            "Epoch 7/10\n",
            "341/341 [==============================] - 4s 11ms/step - loss: 5.1080 - accuracy: 0.1239 - val_loss: 10.6955 - val_accuracy: 0.0716\n",
            "Epoch 8/10\n",
            "341/341 [==============================] - 5s 14ms/step - loss: 4.4751 - accuracy: 0.1661 - val_loss: 11.5074 - val_accuracy: 0.0701\n",
            "Epoch 9/10\n",
            "341/341 [==============================] - 6s 18ms/step - loss: 3.6825 - accuracy: 0.2550 - val_loss: 14.2506 - val_accuracy: 0.0587\n",
            "Epoch 10/10\n",
            "341/341 [==============================] - 7s 19ms/step - loss: 2.8097 - accuracy: 0.3881 - val_loss: 16.7341 - val_accuracy: 0.0565\n",
            "113/113 [==============================] - 1s 8ms/step - loss: 17.0197 - accuracy: 0.0396\n",
            "Test accuracy: 0.03959843888878822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def prepare_sequences(tokenizer, data, sequence_length, vocabulary_size):\n",
        "    # Convert texts to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    # Create predictor sequences and targets\n",
        "    predictor_sequences = []\n",
        "    targets = []\n",
        "    for text in sequences:\n",
        "        for i in range(sequence_length, len(text)):\n",
        "            curr_target = text[i]\n",
        "            curr_predictor_sequence = text[i - sequence_length:i]\n",
        "            predictor_sequences.append(curr_predictor_sequence)\n",
        "            targets.append(curr_target)\n",
        "\n",
        "    # Pad sequences\n",
        "    predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')\n",
        "\n",
        "    # Convert targets to one-hot encoding\n",
        "    target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
        "\n",
        "    return predictor_sequences_padded, target_word_one_hot\n",
        "\n",
        "data = load_dataset(\"ag_news\")\n",
        "\n",
        "data_subset, _ = train_test_split(data[\"train\"][\"text\"], train_size=500)\n",
        "train_data, test_data = train_test_split(data_subset, train_size=0.8)\n",
        "\n",
        "# Prepare the tokenizer and fit on the training text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_subset)\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "\n",
        "sequence_length = 1  # Length of the input sequence before predicting the next word\n",
        "\n",
        "# Prepare training set\n",
        "predictor_sequences_padded, target_word_one_hot = prepare_sequences(tokenizer, train_data, sequence_length, vocabulary_size)\n",
        "\n",
        "# Prepare test set\n",
        "predictor_sequences_padded_test, target_word_one_hot_test = prepare_sequences(tokenizer, test_data, sequence_length, vocabulary_size)\n",
        "\n",
        "# Create the neural language model with SimpleRNN layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
        "model.add(SimpleRNN(100, activation='relu'))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(predictor_sequences_padded, target_word_one_hot, shuffle = False, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(predictor_sequences_padded_test, target_word_one_hot_test)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9vSPCreEAxq",
        "outputId": "6d8ebabc-2714-46ba-ad07-e75cb17302b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5254\n",
            "Epoch 1/10\n",
            "389/389 [==============================] - 8s 16ms/step - loss: 7.7687 - accuracy: 0.0464 - val_loss: 7.5361 - val_accuracy: 0.0454\n",
            "Epoch 2/10\n",
            "389/389 [==============================] - 4s 9ms/step - loss: 6.9330 - accuracy: 0.0541 - val_loss: 7.6850 - val_accuracy: 0.0537\n",
            "Epoch 3/10\n",
            "389/389 [==============================] - 3s 8ms/step - loss: 6.6746 - accuracy: 0.0697 - val_loss: 7.8448 - val_accuracy: 0.0637\n",
            "Epoch 4/10\n",
            "389/389 [==============================] - 3s 7ms/step - loss: 6.4390 - accuracy: 0.0844 - val_loss: 8.0108 - val_accuracy: 0.0721\n",
            "Epoch 5/10\n",
            "389/389 [==============================] - 3s 8ms/step - loss: 6.1729 - accuracy: 0.0987 - val_loss: 8.2095 - val_accuracy: 0.0837\n",
            "Epoch 6/10\n",
            "389/389 [==============================] - 4s 11ms/step - loss: 5.8441 - accuracy: 0.1264 - val_loss: 8.4644 - val_accuracy: 0.0927\n",
            "Epoch 7/10\n",
            "389/389 [==============================] - 3s 8ms/step - loss: 5.4400 - accuracy: 0.1676 - val_loss: 8.8483 - val_accuracy: 0.0985\n",
            "Epoch 8/10\n",
            "389/389 [==============================] - 3s 7ms/step - loss: 4.9698 - accuracy: 0.2080 - val_loss: 9.3449 - val_accuracy: 0.1020\n",
            "Epoch 9/10\n",
            "389/389 [==============================] - 3s 7ms/step - loss: 4.4703 - accuracy: 0.2417 - val_loss: 9.9424 - val_accuracy: 0.1062\n",
            "Epoch 10/10\n",
            "389/389 [==============================] - 4s 11ms/step - loss: 3.9813 - accuracy: 0.2755 - val_loss: 10.6122 - val_accuracy: 0.1040\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 11.0851 - accuracy: 0.0870\n",
            "Test accuracy: 8.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accuracy: 9.78%\n",
        "0.09203854203224182"
      ],
      "metadata": {
        "id": "YHLhxiK0FwTt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDbwUpk49el5"
      },
      "source": [
        "## Reducing your context window\n",
        "\n",
        "Because of the sequential nature of the RNN layer, you don't need to pass in as big of a context window.\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_context_simplification.png?raw=1\" width=500>\n",
        "</div>\n",
        "\n",
        "\n",
        "image source: SLP Fig. 9.5, https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQMhFEBI9el5"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_languagemodeling.png?raw=1\" width=700>\n",
        "</div>\n",
        "\n",
        "### Exercise\n",
        "---\n",
        "Reduce your `sequence_length` to 1. Train and test again.\n",
        "\n",
        "How do the results compare?\n",
        "\n",
        "* it did better with only using one sequence_length it did 9.78%\n",
        "\n",
        "---\n",
        "image source: SLP Fig. 9.6, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX4WOzlS9el5"
      },
      "source": [
        "## Generating Text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Our Keras RNN-based neural language model doesn't do a great job of generating text\n",
        "\n",
        "### Exercise:\n",
        "\n",
        "Try it with this text generation code from last time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2Ue4RTZ9el6",
        "outputId": "797d23e4-d408-42ef-84cf-2d79a33c13ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world cup qualifier set to the world cup qualifier set to the world cup qualifier set to the world cup qualifier set to the world cup qualifier set to the world cup qualifier set to the world cup qualifier set to the world cup qualifier set to the world cup "
          ]
        }
      ],
      "source": [
        "starter_string = \"the\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "tokens = tokens_list[0]\n",
        "\n",
        "for i in range(50):\n",
        "    curr_seq = tokens[-sequence_length:]\n",
        "    curr_array = np.array([curr_seq])\n",
        "    predicted_probabilities = model.predict(curr_array,verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    print(predicted_word+\" \",end=\"\")\n",
        "    tokens.append(predicted_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TN-eIXt9el6"
      },
      "source": [
        "**One problem:** Keras will reset the state every time you make a call to `model.predict` so we lose the benefit of recurrence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69kT1G-F9el7"
      },
      "source": [
        "## Exerting more control over when the state gets reset\n",
        "\n",
        "If your model uses the `stateful=True` parameter on the recurrent layer, you get more control over when to reset the state.\n",
        "* Downside: it's more of a pain to train the network like that\n",
        "\n",
        "*A workaround:* create another model with the same architecture except for `stateful` and copy the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS23EyDo9el7"
      },
      "outputs": [],
      "source": [
        "# Create a new model with the same architecture but with stateful RNNs\n",
        "stateful = Sequential()\n",
        "stateful.add(Embedding(input_dim=vocabulary_size, output_dim=50, batch_input_shape=(1, sequence_length))) #batch size of 1\n",
        "stateful.add(SimpleRNN(100,return_sequences=False,stateful=True))\n",
        "stateful.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# Load the weights from your trained model\n",
        "stateful.set_weights(model.get_weights())\n",
        "\n",
        "# Compile the stateful model (required to make predictions)\n",
        "stateful.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPHRlnSK9el7",
        "outputId": "60f9e605-1036-4af8-ef07-2c76d8c86190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "principles of darfur rebels basketball of manager that could potentially result of music via york without a bridge after the rebound and reduce spending on jan 4 program of music via karzai leads vote count don't cutting maker energy plc shares blair said on jan lt a matter of manager "
          ]
        }
      ],
      "source": [
        "starter_string = \"the\"\n",
        "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
        "tokens = tokens_list[0]\n",
        "\n",
        "#do this anytime you want to reset the states - for generating a brand new sequence\n",
        "stateful.reset_states()\n",
        "\n",
        "for i in range(50):\n",
        "    curr_seq = tokens[-sequence_length:]\n",
        "    curr_array = np.array([curr_seq])\n",
        "    predicted_probabilities = stateful.predict(curr_array,verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    print(predicted_word+\" \",end=\"\")\n",
        "    tokens.append(predicted_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPB0SNqI9el7"
      },
      "source": [
        "## Training a stateful model\n",
        "\n",
        "Keras makes you work a little harder if you want to train a stateful model from the start\n",
        "* Organize your sequences into batches\n",
        "* All batches need to be the same size (say 32 or 64)\n",
        "\n",
        "Might be appropriate if\n",
        "* You have several long documents\n",
        "* Each document takes multiple batches\n",
        "* You *don't* want to reset states between batches\n",
        "* You *do* want to reset states between documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dae4kiJr9el7"
      },
      "source": [
        "## Throwback to a data set we worked with previously\n",
        "\n",
        "This example is going to do a couple of things\n",
        "* use The Adventures of Sherlock Holmes corpus we download from Project Gutenberg\n",
        "* use the WordPiece tokenizer from Hugging Face\n",
        "    * I want to keep around things like punctuation which gets removed by the Keras tokenizer\n",
        "    * I want to show you how you can mix different tokenizers with Keras models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gi9SkRd9el7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "response = requests.get(\"https://www.gutenberg.org/files/1661/1661-0.txt\")\n",
        "sherlock_raw_text = response.text\n",
        "\n",
        "sherlock_tokens = tokenizer.tokenize( sherlock_raw_text )\n",
        "\n",
        "sherlock_tokens = sherlock_tokens[:10000] #let's limit the size of the text for this workshop\n",
        "\n",
        "print(\"Here's a sample of the tokenized text:\")\n",
        "print(sherlock_tokens[1000:1020])\n",
        "\n",
        "token_ids = tokenizer.convert_tokens_to_ids(sherlock_tokens )\n",
        "print(\"\\nHere's the text's ids\")\n",
        "print(token_ids[1000:1020])\n",
        "\n",
        "print(\"Vocabulary size:\")\n",
        "print(len(tokenizer.vocab))\n",
        "vocabulary_size = len(tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "response2 = requests.get(\"https://gutenberg.org/cache/epub/33361/pg33361.txt\")\n",
        "Oz_raw_text = response.text\n",
        "\n",
        "Oz_tokens = tokenizer.tokenize( Oz_raw_text )\n",
        "\n",
        "Oz_tokens = Oz_tokens[:10000] #let's limit the size of the text for this workshop\n",
        "\n",
        "print(\"Here's a sample of the tokenized text:\")\n",
        "print(Oz_tokens[1000:1020])\n",
        "\n",
        "token_ids2 = tokenizer.convert_tokens_to_ids(Oz_tokens )\n",
        "print(\"\\nHere's the text's ids\")\n",
        "print(token_ids2[1000:1020])\n",
        "\n",
        "print(\"Vocabulary size:\")\n",
        "print(len(tokenizer.vocab))\n",
        "vocabulary_size2 = len(tokenizer.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPewvBbILtwP",
        "outputId": "7942ffed-e9ad-4017-ae84-bea139ba7b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (60248 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a sample of the tokenized text:\n",
            "['Bill', '##ina', 'is', '_', 'real', 'Oz', '##zy', '_', ',', 'Mr', '.', 'Ba', '##um', ',', 'and', 'so', 'are', 'T', '##ik', '##tok']\n",
            "\n",
            "Here's the text's ids\n",
            "[2617, 2983, 1110, 168, 1842, 16075, 6482, 168, 117, 1828, 119, 18757, 1818, 117, 1105, 1177, 1132, 157, 4847, 18290]\n",
            "Vocabulary size:\n",
            "28996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNVOYJ9g9el8"
      },
      "source": [
        "### Preparing the list of predictor/target pairs like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwL4Wro29el8"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "sequence_length = 1\n",
        "batch_size = 32\n",
        "\n",
        "predictor_sequences = []\n",
        "targets = []\n",
        "for i in range(sequence_length, len(token_ids)):\n",
        "    # Take the sequence of tokens as input and the next token as target\n",
        "    curr_target = token_ids[i]\n",
        "    curr_predictor_sequence = token_ids[i-sequence_length:i]\n",
        "    predictor_sequences.append(curr_predictor_sequence)\n",
        "    targets.append(curr_target)\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "targets_one_hot = to_categorical(targets, num_classes=vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequence_length = 1\n",
        "batch_size = 32\n",
        "\n",
        "predictor_sequences = []\n",
        "targets = []\n",
        "for i in range(sequence_length, len(token_ids2)):\n",
        "    # Take the sequence of tokens as input and the next token as target\n",
        "    curr_target2 = token_ids[i]\n",
        "    curr_predictor_sequence2 = token_ids[i-sequence_length:i]\n",
        "    predictor_sequences.append(curr_predictor_sequence2)\n",
        "    targets.append(curr_target2)\n",
        "\n",
        "# Convert target to one-hot encoding\n",
        "targets_one_hot2 = to_categorical(targets, num_classes=vocabulary_size2)"
      ],
      "metadata": {
        "id": "22LPtF5DMv2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVdqDz799el8"
      },
      "source": [
        "### Grouping the sequences into batches of 32\n",
        "\n",
        "This adds an extra dimension to our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpKM_j3U9el8"
      },
      "outputs": [],
      "source": [
        "def put_into_batches(data,batch_size):\n",
        "    num_batches = (len(data) // batch_size)\n",
        "    batched_data = []\n",
        "    for batch_idx in range(num_batches):\n",
        "        curr_batch = data[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
        "        batched_data.append(curr_batch)\n",
        "    batched_data = np.array(batched_data)\n",
        "    return batched_data\n",
        "\n",
        "\n",
        "train_features_batched = put_into_batches(predictor_sequences,batch_size)\n",
        "train_targets_batched = put_into_batches(targets_one_hot,batch_size)\n",
        "\n",
        "print(\"before batching\")\n",
        "print(np.array(predictor_sequences))\n",
        "\n",
        "print(\"\\nafter batching\")\n",
        "print(train_features_batched)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batched2 = put_into_batches(predictor_sequences,batch_size)\n",
        "train_targets_batched2 = put_into_batches(targets_one_hot2,batch_size)\n",
        "\n",
        "print(\"before batching\")\n",
        "print(np.array(predictor_sequences))\n",
        "\n",
        "print(\"\\nafter batching\")\n",
        "print(train_features_batched2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tnZxCgGPVgo",
        "outputId": "53fe27c8-77ed-44b2-ee9f-041a6fcbd262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before batching\n",
            "[[1109]\n",
            " [4042]\n",
            " [ 144]\n",
            " ...\n",
            " [3219]\n",
            " [1123]\n",
            " [1246]]\n",
            "\n",
            "after batching\n",
            "[[[ 1109]\n",
            "  [ 4042]\n",
            "  [  144]\n",
            "  ...\n",
            "  [ 1168]\n",
            "  [ 2192]\n",
            "  [ 1104]]\n",
            "\n",
            " [[ 1103]\n",
            "  [ 1362]\n",
            "  [ 1120]\n",
            "  ...\n",
            "  [ 1103]\n",
            "  [ 4042]\n",
            "  [  144]]\n",
            "\n",
            " [[ 6140]\n",
            "  [ 8904]\n",
            "  [24689]\n",
            "  ...\n",
            "  [ 1209]\n",
            "  [ 1138]\n",
            "  [ 1106]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1117]\n",
            "  [ 1532]\n",
            "  [ 1122]\n",
            "  ...\n",
            "  [ 1942]\n",
            "  [18589]\n",
            "  [26140]]\n",
            "\n",
            " [[13020]\n",
            "  [18589]\n",
            "  [ 2162]\n",
            "  ...\n",
            "  [  107]\n",
            "  [  146]\n",
            "  [ 1267]]\n",
            "\n",
            " [[ 1122]\n",
            "  [ 2762]\n",
            "  [  112]\n",
            "  ...\n",
            "  [ 1513]\n",
            "  [ 1107]\n",
            "  [ 1103]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK-LWXcl9el8"
      },
      "source": [
        "## Creating and compiling the model\n",
        "\n",
        "Note that in this case, we set `batch_input_shape=(batch_size, sequence_length)`\n",
        "\n",
        "instead of `input_length=sequence_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiz3WcEK9el9"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, batch_input_shape=(batch_size, sequence_length)))\n",
        "model.add(SimpleRNN(100,return_sequences=False,stateful=True))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAHdeJg29el9"
      },
      "source": [
        "## Writing a training loop\n",
        "\n",
        "instead of just doing `model.fit`, we'll do `model.train_on_batch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPUeN9yh9el9",
        "outputId": "4ad1cda2-9563-4cca-910f-a140f28a87e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 2/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 3/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 4/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 5/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 6/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 7/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 8/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 9/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n",
            "Epoch 10/10\n",
            "\tBatch 100/312\n",
            "\tBatch 200/312\n",
            "\tBatch 300/312\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10  # Number of epochs to train for\n",
        "number_of_batches = len(train_features_batched)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    model.reset_states()  # Reset states at the start of each epoch\n",
        "\n",
        "\n",
        "    for batch_idx in range(number_of_batches):\n",
        "        #print batch number every 1000 batches\n",
        "        if (batch_idx+1) % 100 == 0:\n",
        "            print(f'\\tBatch {batch_idx+1}/{number_of_batches}')\n",
        "\n",
        "        # Train on the batch\n",
        "        model.train_on_batch(train_features_batched[batch_idx], train_targets_batched[batch_idx])\n",
        "\n",
        "\n",
        "\n",
        " # if you switch to a new document, do this\n",
        "    model.reset_states()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJO2F5LC9el9"
      },
      "source": [
        "### Now let's use our model to generate some text\n",
        "\n",
        "This code looks much different because we're using the Hugging Face tokenizer\n",
        "* turn text into ids with `tokenizer.encode`\n",
        "* turn ids into text with `tokenizer.decode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxxfqcWf9el-",
        "outputId": "5dd25137-6950-4265-f7f8-e6082ab059c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theut the man, and theut the man, and theut the man, and theut the man, and theut the man, and theut the man, and theut the man, and theut the man, and theut the\n"
          ]
        }
      ],
      "source": [
        "starter_string = \"the\"\n",
        "\n",
        "# Encode the starter string to token IDs\n",
        "input_ids = tokenizer.encode(starter_string, add_special_tokens=False)\n",
        "\n",
        "for i in range(50):\n",
        "    # Get the last sequence_length tokens\n",
        "    curr_seq = input_ids[-sequence_length:]\n",
        "    # Predict the next token ID\n",
        "    predicted_probabilities = model.predict(np.array([curr_seq]), verbose=0)\n",
        "    predicted_index = np.argmax(predicted_probabilities, axis=-1)\n",
        "    # Add the predicted token ID to the sequence\n",
        "    input_ids.append(predicted_index[0])\n",
        "\n",
        "# Decode the token IDs to a string\n",
        "generated_sequence = tokenizer.decode(input_ids, clean_up_tokenization_spaces=True)\n",
        "print(generated_sequence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Qa5AZN9el-"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "Adjust the code to get this working on more than one longer document\n",
        "* can get multiple Project Gutenberg texts\n",
        "* can use a Hugging Face dataset with longer texts (i.e., multiple sentences per entry, unlike `ag_news`)\n",
        "\n",
        "Let it train for a while and then generate some text\n",
        "* Did training with larger data sets improve the kind of text you were able to generate?\n",
        "* describe what you did and write up an interpretation of your results"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}